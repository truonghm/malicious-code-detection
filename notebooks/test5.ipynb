{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchtext\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pytorch_lightning as pl\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits, binary_cross_entropy\n",
    "from torchmetrics import Accuracy, F1Score\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/malicious-code-detection\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.chdir(os.getenv(\"PROJECT_ROOT_DIR\"))\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MODEL_EVAL_METRIC:\n",
    "    accuracy = \"accuracy\"\n",
    "    f1_score = \"f1_score\"\n",
    "\n",
    "class Config:\n",
    "    VOCAB_SIZE = 0\n",
    "    BATCH_SIZE = 2\n",
    "    EMB_SIZE = 100\n",
    "    OUT_SIZE = 2\n",
    "    NUM_FOLDS = 5\n",
    "    NUM_EPOCHS = 10\n",
    "    NUM_WORKERS = 8\n",
    "    # Whether to update the pretrained embedding weights during training process\n",
    "    EMB_WT_UPDATE = True\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    MODEL_EVAL_METRIC = MODEL_EVAL_METRIC.accuracy\n",
    "    FAST_DEV_RUN = False    \n",
    "    PATIENCE = 6    \n",
    "    IS_BIDIRECTIONAL = True\n",
    "    # model hyperparameters\n",
    "    MODEL_PARAMS = {\n",
    "        \"hidden_size\": 141, \n",
    "        \"num_layers\": 2,         \n",
    "        \"drop_out\": 0.4258,\n",
    "        \"lr\": 0.000366,\n",
    "        \"weight_decay\": 0.00001\n",
    "    }\n",
    "    X_TEST_PATH = 'data/exp/test_set_token_types_corpus.txt'\n",
    "    Y_TEST_PATH = 'data/exp/test_set_labels.txt'\n",
    "    X_TRAIN_PATH = 'data/exp/train_set_token_types_corpus.txt'\n",
    "    Y_TRAIN_PATH = 'data/exp/train_set_labels.txt' \n",
    "    \n",
    "# For results reproducibility \n",
    "# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\n",
    "pl.seed_everything(42, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "\twith open(path, \"r\") as f:\n",
    "\t\tdata = f.readlines()\n",
    "\t\tvectors = [token.split() for token in data]\n",
    "\t\treturn vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = read_data(Config.X_TRAIN_PATH)\n",
    "y_train_str = np.loadtxt(Config.Y_TRAIN_PATH, dtype=str)\n",
    "y_train = np.where(y_train_str == 'goodjs', 0.0, 1.0)\n",
    "\n",
    "X_test = read_data(Config.X_TEST_PATH)\n",
    "y_test_str = np.loadtxt(Config.Y_TEST_PATH, dtype=str)\n",
    "y_test = np.where(y_test_str == 'goodjs', 0.0, 1.0)\n",
    "\n",
    "df_train = pd.DataFrame({'X': X_train, 'y': y_train})\n",
    "df_train = df_train[df_train[\"X\"].apply(len) != 0]\n",
    "df_test = pd.DataFrame({'X': X_test, 'y': y_test})\n",
    "df_test = df_test[df_test[\"X\"].apply(len) != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a sample of df_train and df_test only, stratify on y\n",
    "\n",
    "df_train = df_train.sample(n=80, random_state=42)\n",
    "df_test = df_test.sample(n=20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique vocab size: 9\n"
     ]
    }
   ],
   "source": [
    "unique_vocabs = set()\n",
    "for x in X_train:\n",
    "\tunique_vocabs.update(x)\n",
    "\n",
    "print(f\"Unique vocab size: {len(unique_vocabs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strat_kfold_dataframe(df, target_col_name, num_folds=5):\n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    df[\"kfold\"] = -1\n",
    "    # randomize of shuffle the rows of dataframe before splitting is done\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    # get the target data\n",
    "    y = df[target_col_name].values\n",
    "    skf = model_selection.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X=df, y=y)):\n",
    "        df.loc[val_index, \"kfold\"] = fold\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = strat_kfold_dataframe(df_train, target_col_name=\"y\", num_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTTEXT_EMB_FILE = \"models/word_rep_by_types.vec\"\n",
    "emb = torchtext.vocab.Vectors(name=FASTTEXT_EMB_FILE, cache=\"./vector_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(df):\n",
    "    for index, row in df.iterrows():\n",
    "        yield row[\"X\"]\n",
    "    \n",
    "ast_vocab = build_vocab_from_iterator(yield_tokens(df_train), specials=[\"<unk>\", \"<pad>\"])   \n",
    "Config.VOCAB_SIZE = len(ast_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_pt_emb_matrix(text_vocab, emb):\n",
    "    embedding_matrix = []\n",
    "    for token in text_vocab.get_itos():\n",
    "        embedding_matrix.append(emb.get_vecs_by_tokens(token))\n",
    "    return torch.stack(embedding_matrix)\n",
    "\n",
    "pt_emb_weights = get_vocab_pt_emb_matrix(ast_vocab, emb)\n",
    "pt_emb_layer = nn.Embedding.from_pretrained(pt_emb_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"vectorized_X\"] = df_train[\"X\"].apply(\n",
    "    lambda row:torch.LongTensor(ast_vocab.lookup_indices(row))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JavaScriptASTDataset(Dataset):\n",
    "    def __init__(self, ast_vecs, labels):\n",
    "        self.ast_vecs = ast_vecs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ast_vec = self.ast_vecs[idx]\n",
    "        label = self.labels[idx]\n",
    "        # ast_len = len(ast_vec)\n",
    "        return (ast_vec, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "    # Each element in the batch is a tuple (data, label)\n",
    "    # sort the batch (based on tweet word count) in descending order\n",
    "    sorted_batch = sorted(batch, key=lambda x:x[0].shape[0], reverse=True)\n",
    "    sequences = [x[0] for x in sorted_batch]\n",
    "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=0)\n",
    "    # Also need to store the length of each sequence.This is later needed in order to unpad \n",
    "    # the sequences\n",
    "    seq_len = torch.Tensor([len(x) for x in sequences])\n",
    "    labels = torch.Tensor([x[1] for x in sorted_batch])\n",
    "    return sequences_padded, seq_len, labels\n",
    "\n",
    "def get_fold_dls(fold, df):\n",
    "    train_df = df[df.kfold != fold].reset_index(drop=True)\n",
    "    valid_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "    X_train = train_df[\"vectorized_X\"].to_numpy()\n",
    "    y_train = train_df[\"y\"].to_numpy()\n",
    "    X_valid = valid_df[\"vectorized_X\"].to_numpy()\n",
    "    y_valid = valid_df[\"y\"].to_numpy()\n",
    "    ds_train = JavaScriptASTDataset(X_train, y_train)\n",
    "    ds_valid = JavaScriptASTDataset(X_valid, y_valid)\n",
    "    dl_train = DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True, collate_fn=pad_collate, num_workers=Config.NUM_WORKERS)\n",
    "    dl_valid = DataLoader(ds_valid, batch_size=Config.BATCH_SIZE, collate_fn=pad_collate, num_workers=Config.NUM_WORKERS)\n",
    "    return dl_train, dl_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class DisasterModel(nn.Module):\n",
    "    \"\"\"The RNN model.\"\"\"\n",
    "    def __init__(self, vocab_size, num_layers, is_bidirect, emb_size, hidden_size, output_size, \n",
    "                pt_emb_weights, emb_wt_update=False, drop_prob=0.5):\n",
    "        super().__init__()        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers        \n",
    "        # size of the embedding vector\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size   \n",
    "        self.output_dim = output_size\n",
    "        self.is_bidirect = is_bidirect\n",
    "        # Embedding layer\n",
    "        self.emb_layer = nn.Embedding(self.vocab_size, emb_size)\n",
    "        # copy the vocab specific weights(emb vectors) from pretrained embeddings to model embedding layer\n",
    "        self.emb_layer.weight.data.copy_(pt_emb_weights)    \n",
    "        # whether to update the pretrained embedding layer weights during model training\n",
    "        self.emb_layer.weight.requires_grad = emb_wt_update            \n",
    "        # LSTM Layer        \n",
    "        self.lstm_layer = nn.LSTM(\n",
    "                        input_size=emb_size, \n",
    "                        hidden_size=hidden_size, \n",
    "                        batch_first=True, \n",
    "                        bidirectional=is_bidirect, \n",
    "                        num_layers=num_layers, \n",
    "                        dropout=drop_prob\n",
    "                        )\n",
    "        self.dropout = nn.Dropout(p = drop_prob)                        \n",
    "        \n",
    "        # If the RNN is bidirectional `num_directions` should be 2, else it should be 1.        \n",
    "        if not is_bidirect:\n",
    "            self.num_directions = 1\n",
    "            self.linear = nn.Linear(self.hidden_size, self.output_dim)\n",
    "        else:       \n",
    "            self.num_directions = 2     \n",
    "            self.linear = nn.Linear(self.hidden_size * self.num_directions, self.output_dim)\n",
    "        # The activation layer which converts output to 0 or 1            \n",
    "        self.act = nn.Sigmoid()            \n",
    "\n",
    "    def forward(self, inputs, input_lengths, state):        \n",
    "        # inputs = [batch_size, batch_max_seq_length]        \n",
    "        # embeds is of shape batch_size * num_steps * emb_dim and is the input to lstm layer\n",
    "        embeds = self.emb_layer(inputs)        \n",
    "        batch_size = inputs.shape[0]        \n",
    "        # embeds = [batch_size, max_seq_length, emb_dim]\n",
    "        # pack_padded_sequence before feeding into LSTM. This is required so pytorch knows\n",
    "        # which elements of the sequence are padded ones and ignore them in computation.\n",
    "        # This step is done only after the embedding step\n",
    "        embeds_pack = pack_padded_sequence(embeds, input_lengths.to(\"cpu\"), batch_first=True)                \n",
    "        lstm_out_pack, (h_n, c_n) = self.lstm_layer(embeds_pack)\n",
    "        # h_n and c_n = [num_directions * num_layers, batch_size, hidden_size]\n",
    "        # unpack the output\n",
    "        lstm_out, lstm_out_len = pad_packed_sequence(lstm_out_pack, batch_first=True)        \n",
    "        #print(f\"lstm_out.shape = {lstm_out.shape}\")\n",
    "        #print(f\"lstm_out_len.shape = {lstm_out_len.shape}\")\n",
    "        # lstm_out = [batch_size, max_seq_length, hidden_size * num_directions]\n",
    "        if self.is_bidirect:            \n",
    "            # each batch item has different seq length, so to select the hidden state at t_end for each batch item\n",
    "            # a for comprehension like below is needed, a vectorized operation doesn't seem plausible\n",
    "            #lstm_out = [lstm_out[batch_item_index, seq_length_index-1, :] for batch_item_index, seq_length_index in enumerate(lstm_out_len)]            \n",
    "            #lstm_out = torch.cat(lstm_out, dim=0).reshape(batch_size, 4 * self.hidden_size)\n",
    "            #print(f\"lstm_out.shape = {lstm_out.shape}\")\n",
    "            # Another way to extract the last hidden state for the forward and backward lstm layers\n",
    "            # in a BiRNN is to use h_n like this\n",
    "            h_tend_fwd = h_n[-2, :, :]\n",
    "            h_tend_bwd = h_n[-1, :, :]\n",
    "            lstm_out = torch.cat((h_tend_fwd, h_tend_bwd), dim=1)\n",
    "            #print(f\"lstm_out.shape = {lstm_out.shape}\")\n",
    "        else:                        \n",
    "            lstm_out = h_n[-1, :, :]                    \n",
    "        \n",
    "        out = self.dropout(lstm_out)                \n",
    "        output = self.linear(out)        \n",
    "        # apply sigmoid activation to convert output to probability \n",
    "        output = self.act(output)\n",
    "        # [batch_size, 2]\n",
    "        return output\n",
    "\n",
    "    def init_state(self, batch_size=1):\n",
    "        \"\"\" Initialize the hidden state i.e. initialize all the neurons in all the hidden layers \n",
    "        to zero\"\"\"\n",
    "        if not isinstance(self.lstm_layer, nn.LSTM):\n",
    "            # `nn.GRU` takes a tensor as hidden state\n",
    "            return torch.zeros((self.num_directions * self.num_layers, batch_size, self.hidden_size))\n",
    "        else:\n",
    "            # `nn.LSTM` takes a tuple of hidden states (h0, c0). h0 = initial\n",
    "            # hidden state for each element in the batch, c0 = initial cell state\n",
    "            # for each element in the batch\n",
    "            return (torch.zeros((self.num_directions * self.num_layers, batch_size, self.hidden_size)),\n",
    "                    torch.zeros((self.num_directions * self.num_layers,batch_size, self.hidden_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisasterTweetLitModel(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, emb_size, output_size, pt_emb_weights, emb_wt_update, \n",
    "                hparams, model_eval_metric=MODEL_EVAL_METRIC.accuracy):\n",
    "        super().__init__()\n",
    "        #self.save_hyperparameters()\n",
    "        self.lr = hparams[\"lr\"]\n",
    "        self.weight_decay = hparams[\"weight_decay\"]\n",
    "        self.model_eval_metric = model_eval_metric\n",
    "        self.network = DisasterModel(\n",
    "            vocab_size = vocab_size,\n",
    "            num_layers = hparams[\"num_layers\"],\n",
    "            is_bidirect = Config.IS_BIDIRECTIONAL,\n",
    "            emb_size = emb_size,\n",
    "            hidden_size = hparams[\"hidden_size\"],\n",
    "            output_size = output_size,\n",
    "            pt_emb_weights = pt_emb_weights,\n",
    "            emb_wt_update = emb_wt_update,\n",
    "            drop_prob = hparams[\"drop_out\"]\n",
    "        )\n",
    "\n",
    "    def forward(self, tweets, tweet_lengths, state):\n",
    "        return self.network(tweets, tweet_lengths, state)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        model_optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(model_optimizer, mode=\"min\")\n",
    "        return {\n",
    "            \"optimizer\": model_optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"monitor\": \"val_loss\",\n",
    "                \"frequency\": 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        tweets, tweet_lengths, targets = batch\n",
    "        # initialize the hidden and cell state of the LSTM\n",
    "        h0, c0 = self.network.init_state()\n",
    "        targets_pred = self(tweets, tweet_lengths, (h0, c0))        \n",
    "        #print(f\"targets_pred.shape = {targets_pred.shape}\")\n",
    "        loss_targets = F.one_hot(targets.T.long(), num_classes=2)\n",
    "        loss_targets = loss_targets.float()        \n",
    "        train_loss = binary_cross_entropy(targets_pred, loss_targets)\n",
    "        train_metric = None\n",
    "        train_metric_str = \"\"\n",
    "        if self.model_eval_metric == MODEL_EVAL_METRIC.accuracy:            \n",
    "            targets_pred = torch.argmax(targets_pred, dim=1)            \n",
    "            train_metric = Accuracy(task=\"binary\", num_classes=2)(targets_pred.cpu(), targets.long().cpu())\n",
    "            train_metric_str = \"train_acc\"\n",
    "        elif self.model_eval_metric == MODEL_EVAL_METRIC.f1_score:\n",
    "            train_metric = F1(targets_pred, targets)            \n",
    "            train_metric_str = \"train_f1\"\n",
    "        self.log(\"train_loss\", train_loss, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n",
    "        self.log(train_metric_str, train_metric, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n",
    "        return train_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        tweets, tweet_lengths, targets = batch\n",
    "        # initialize the hidden and cell state of the LSTM\n",
    "        h0, c0 = self.network.init_state()\n",
    "        targets_pred = self(tweets, tweet_lengths, (h0, c0))\n",
    "        loss_targets = F.one_hot(targets.T.long(), num_classes=2)\n",
    "        loss_targets = loss_targets.float()        \n",
    "        val_loss = binary_cross_entropy(targets_pred, loss_targets)\n",
    "        val_metric = None\n",
    "        val_metric_str = \"\"\n",
    "        if self.model_eval_metric == MODEL_EVAL_METRIC.accuracy:\n",
    "            targets_pred = torch.argmax(targets_pred, dim=1)\n",
    "            val_metric = Accuracy(task=\"binary\", num_classes=2)(targets_pred.cpu(), targets.long().cpu())\n",
    "            val_metric_str = \"val_acc\"\n",
    "        elif self.model_eval_metric == MODEL_EVAL_METRIC.f1_score:\n",
    "            val_metric = F1(targets_pred, targets)            \n",
    "            val_metric_str = \"val_f1\"\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n",
    "        self.log(val_metric_str, val_metric, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n",
    "        return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "# Monitor multiple metric values that are calculated either in training or validation step and return the\n",
    "# best metric values for each epoch\n",
    "class MetricsAggCallback(Callback):\n",
    "    def __init__(self, train_metrics_to_monitor, val_metrics_to_monitor):\n",
    "        # dictionary with metric name as key and monitor mode (min, max) as the value\n",
    "        # ( the same names used to log metric values in training and validation step)\n",
    "        self.val_metrics_to_monitor = val_metrics_to_monitor\n",
    "        self.train_metrics_to_monitor = train_metrics_to_monitor\n",
    "        # dictionary with metric_name as key and list of metric value for each epoch\n",
    "        self.train_metrics = {metric: [] for metric in train_metrics_to_monitor.keys()}\n",
    "        self.val_metrics = {metric: [] for metric in val_metrics_to_monitor.keys()}\n",
    "        # dictionary with metric_name as key and the best metric value for all epochs\n",
    "        self.train_best_metric = {metric: None for metric in train_metrics_to_monitor.keys()}\n",
    "        self.val_best_metric = {metric: None for metric in val_metrics_to_monitor.keys()}\n",
    "        # dictionary with metric_name as key and the epoch number with the best metric value\n",
    "        self.train_best_metric_epoch = {metric: None for metric in train_metrics_to_monitor.keys()}     \n",
    "        self.val_best_metric_epoch = {metric: None for metric in val_metrics_to_monitor.keys()}     \n",
    "        self.epoch_counter = 0           \n",
    "\n",
    "    @staticmethod\n",
    "    def process_metrics(metrics_to_monitor, metrics, best_metric, best_metric_epoch, trainer):\n",
    "        metric_str = \"\"\n",
    "        for metric, mode in metrics_to_monitor.items():\n",
    "            metric_value = round(trainer.callback_metrics[metric].cpu().detach().item(), 4)            \n",
    "            metric_str += f\"{metric} = {metric_value}, \"\n",
    "            metrics[metric].append(metric_value)\n",
    "            if mode == \"max\":\n",
    "                best_metric[metric] = max(metrics[metric])            \n",
    "            elif mode == \"min\":            \n",
    "                best_metric[metric] = min(metrics[metric])            \n",
    "            best_metric_epoch[metric] = metrics[metric].index(best_metric[metric]) \n",
    "        print(metric_str[:-2])\n",
    "\n",
    "    def on_train_epoch_end(self, trainer: Trainer, pl_module: LightningModule):\n",
    "        self.epoch_counter += 1        \n",
    "        self.process_metrics(self.train_metrics_to_monitor, self.train_metrics, self.train_best_metric, self.train_best_metric_epoch, trainer)\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer: Trainer, pl_module: LightningModule):        \n",
    "        print(f\"For epoch {self.epoch_counter}\")\n",
    "        self.process_metrics(self.val_metrics_to_monitor, self.val_metrics, self.val_best_metric, self.val_best_metric_epoch, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(fold, dl_train, dl_val, pt_emb_weights, find_lr=True):\n",
    "    fold_str = f\"fold{fold}\"\n",
    "    print(f\"Running training for {fold_str}\")\n",
    "    disaster_tweet_model = DisasterTweetLitModel(\n",
    "        vocab_size=Config.VOCAB_SIZE,\n",
    "        emb_size=Config.EMB_SIZE,\n",
    "        output_size=Config.OUT_SIZE,\n",
    "        pt_emb_weights=pt_emb_weights,\n",
    "        emb_wt_update=Config.EMB_WT_UPDATE,\n",
    "        hparams=Config.MODEL_PARAMS,\n",
    "        model_eval_metric=Config.MODEL_EVAL_METRIC                \n",
    "        )\n",
    "    tb_logger = pl.loggers.TensorBoardLogger(save_dir=\"logs\")    \n",
    "    chkpt_file_name = fold_str + \"_best_model_{epoch}_{val_loss:.4f}\"\n",
    "    train_metrics_to_monitor = {\n",
    "        \"train_loss\": \"min\",\n",
    "        \"train_acc\": \"max\"\n",
    "    }\n",
    "    val_metrics_to_monitor = {\n",
    "        \"val_loss\": \"min\",\n",
    "        \"val_acc\": \"max\",\n",
    "        }\n",
    "    loss_chkpt_callback = ModelCheckpoint(dirpath=\"./model\", verbose=True, monitor=\"val_loss\", mode=\"min\", filename=chkpt_file_name)    \n",
    "    metric_chkpt_callback = MetricsAggCallback(train_metrics_to_monitor, val_metrics_to_monitor)\n",
    "    early_stopping_callback = EarlyStopping(monitor=\"val_loss\", patience=Config.PATIENCE, mode=\"min\", verbose=True)\n",
    "    trainer = pl.Trainer(\n",
    "        # gpus = 1,\n",
    "        accelerator=\"auto\",\n",
    "        deterministic = True,\n",
    "        # auto_select_gpus = True,\n",
    "        # progress_bar_refresh_rate = 20,\n",
    "        max_epochs = Config.NUM_EPOCHS,\n",
    "        logger = tb_logger,\n",
    "        # auto_lr_find = True,    \n",
    "        #precision = Config.PRECISION,   \n",
    "        fast_dev_run = Config.FAST_DEV_RUN, \n",
    "        gradient_clip_val = 1.0,        \n",
    "        callbacks = [loss_chkpt_callback, metric_chkpt_callback, early_stopping_callback]\n",
    "    )        \n",
    "    if find_lr:\n",
    "        trainer.tune(model=disaster_tweet_model, train_dataloaders=dl_train)\n",
    "        print(disaster_tweet_model.lr)\n",
    "    trainer.fit(disaster_tweet_model, train_dataloaders=dl_train, val_dataloaders=dl_val)\n",
    "    fold_train_metrics = {\n",
    "        metric: (metric_chkpt_callback.train_best_metric[metric], metric_chkpt_callback.train_best_metric_epoch[metric]) \n",
    "        for metric in train_metrics_to_monitor.keys()\n",
    "    }\n",
    "    fold_val_metrics = {\n",
    "        metric: (metric_chkpt_callback.val_best_metric[metric], metric_chkpt_callback.val_best_metric_epoch[metric]) \n",
    "        for metric in val_metrics_to_monitor.keys()\n",
    "    }            \n",
    "    best_model = loss_chkpt_callback.best_model_path\n",
    "    del trainer, disaster_tweet_model, loss_chkpt_callback, metric_chkpt_callback \n",
    "    return fold_train_metrics, fold_val_metrics, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training for fold0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /root/malicious-code-detection/model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type          | Params\n",
      "------------------------------------------\n",
      "0 | network | DisasterModel | 755 K \n",
      "------------------------------------------\n",
      "755 K     Trainable params\n",
      "0         Non-trainable params\n",
      "755 K     Total params\n",
      "3.021     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983a3b490f224f3ba664957606f907e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_94495/3264836254.py:63: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1682343967769/work/aten/src/ATen/native/TensorShape.cpp:3571.)\n",
      "  loss_targets = F.one_hot(targets.T.long(), num_classes=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0\n",
      "val_loss = 0.6946, val_acc = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (32) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde2711785d74448a93cb3441cacdb48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7657eb762344e42865f7204ba669626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.689\n",
      "Epoch 0, global step 32: 'val_loss' reached 0.68894 (best 0.68894), saving model to '/root/malicious-code-detection/model/fold0_best_model_epoch=0_val_loss=0.6889.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0\n",
      "val_loss = 0.6889, val_acc = 0.5625\n",
      "train_loss = 0.694, train_acc = 0.4844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c109f2aa6ff40bcba5c85c14f08c9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.688\n",
      "Epoch 1, global step 64: 'val_loss' reached 0.68819 (best 0.68819), saving model to '/root/malicious-code-detection/model/fold0_best_model_epoch=1_val_loss=0.6882.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 1\n",
      "val_loss = 0.6882, val_acc = 0.5625\n",
      "train_loss = 0.6845, train_acc = 0.6094\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "368faf61708e4c3395d07441dc381cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.685\n",
      "Epoch 2, global step 96: 'val_loss' reached 0.68546 (best 0.68546), saving model to '/root/malicious-code-detection/model/fold0_best_model_epoch=2_val_loss=0.6855.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 2\n",
      "val_loss = 0.6855, val_acc = 0.5625\n",
      "train_loss = 0.6738, train_acc = 0.5469\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb60a64ff20943e99b210a4f49b7d7ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 128: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 3\n",
      "val_loss = 0.6942, val_acc = 0.5625\n",
      "train_loss = 0.6696, train_acc = 0.5781\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1858c285a749c5813a8f1dd90633c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 160: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 4\n",
      "val_loss = 0.7168, val_acc = 0.5625\n",
      "train_loss = 0.6652, train_acc = 0.625\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e9f49224324ce09a4ed4b6d2093594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 192: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 5\n",
      "val_loss = 0.7581, val_acc = 0.4375\n",
      "train_loss = 0.6505, train_acc = 0.6562\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e706efdb918e40daa95605bb189cd298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 224: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 6\n",
      "val_loss = 0.7054, val_acc = 0.5\n",
      "train_loss = 0.6444, train_acc = 0.6562\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95fc0e5a83e044b2bf4bdbc4ba0bdd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 256: 'val_loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 7\n",
      "val_loss = 0.7519, val_acc = 0.4375\n",
      "train_loss = 0.6558, train_acc = 0.6406\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddec0bff626447f89ecce5a1a31c9d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.027 >= min_delta = 0.0. New best score: 0.658\n",
      "Epoch 8, global step 288: 'val_loss' reached 0.65829 (best 0.65829), saving model to '/root/malicious-code-detection/model/fold0_best_model_epoch=8_val_loss=0.6583.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 8\n",
      "val_loss = 0.6583, val_acc = 0.6875\n",
      "train_loss = 0.6168, train_acc = 0.6562\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41f700669e244d287e16fa35ba1d770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.019 >= min_delta = 0.0. New best score: 0.640\n",
      "Epoch 9, global step 320: 'val_loss' reached 0.63977 (best 0.63977), saving model to '/root/malicious-code-detection/model/fold0_best_model_epoch=9_val_loss=0.6398.ckpt' as top 1\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 9\n",
      "val_loss = 0.6398, val_acc = 0.625\n",
      "train_loss = 0.6109, train_acc = 0.6406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type          | Params\n",
      "------------------------------------------\n",
      "0 | network | DisasterModel | 755 K \n",
      "------------------------------------------\n",
      "755 K     Trainable params\n",
      "0         Non-trainable params\n",
      "755 K     Total params\n",
      "3.021     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best train metrics values for fold0\n",
      "{'train_loss': (0.6109, 9), 'train_acc': (0.6562, 5)}\n",
      "Best val metrics values for fold0\n",
      "{'val_loss': (0.6398, 10), 'val_acc': (0.6875, 9)}\n",
      "Running training for fold1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1473b42983904d3d8c0bfc964c435b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0\n",
      "val_loss = 0.6993, val_acc = 0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a161d51526424889a688d346619f32a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3f9695fcba429b910fa5dd58bf1975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.682\n",
      "Epoch 0, global step 32: 'val_loss' reached 0.68227 (best 0.68227), saving model to '/root/malicious-code-detection/model/fold1_best_model_epoch=0_val_loss=0.6823.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0\n",
      "val_loss = 0.6823, val_acc = 0.5\n",
      "train_loss = 0.6918, train_acc = 0.5469\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5210f4b5b2d4e7fa5b4ca2d8309ce52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.007 >= min_delta = 0.0. New best score: 0.676\n",
      "Epoch 1, global step 64: 'val_loss' reached 0.67551 (best 0.67551), saving model to '/root/malicious-code-detection/model/fold1_best_model_epoch=1_val_loss=0.6755.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 1\n",
      "val_loss = 0.6755, val_acc = 0.5\n",
      "train_loss = 0.6885, train_acc = 0.5938\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69cf8f51ffa8419e9c5fee3896d218cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.675\n",
      "Epoch 2, global step 96: 'val_loss' reached 0.67467 (best 0.67467), saving model to '/root/malicious-code-detection/model/fold1_best_model_epoch=2_val_loss=0.6747.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 2\n",
      "val_loss = 0.6747, val_acc = 0.5625\n",
      "train_loss = 0.6699, train_acc = 0.5781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type          | Params\n",
      "------------------------------------------\n",
      "0 | network | DisasterModel | 755 K \n",
      "------------------------------------------\n",
      "755 K     Trainable params\n",
      "0         Non-trainable params\n",
      "755 K     Total params\n",
      "3.021     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best train metrics values for fold1\n",
      "{'train_loss': (0.6699, 2), 'train_acc': (0.5938, 1)}\n",
      "Best val metrics values for fold1\n",
      "{'val_loss': (0.6747, 3), 'val_acc': (0.5625, 3)}\n",
      "Running training for fold2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277b80be2de84f86bfda950d8cc44d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 0\n",
      "val_loss = 0.675, val_acc = 1.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea840d987c3a406ca4c9bd04f3e299f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "find_lr = True\n",
    "all_fold_val_loss = []\n",
    "all_fold_val_acc = []\n",
    "\n",
    "for fold in range(Config.NUM_FOLDS):\n",
    "    dl_train, dl_val = get_fold_dls(fold, df_train)\n",
    "    fold_train_metrics, fold_val_metrics, chkpt_file_name = run_training(fold, dl_train, dl_val, pt_emb_weights, find_lr=False)    \n",
    "    all_fold_val_loss.append((fold_val_metrics[\"val_loss\"][0], chkpt_file_name))\n",
    "    all_fold_val_acc.append(fold_val_metrics[\"val_acc\"][0])\n",
    "    print(f\"Best train metrics values for fold{fold}\")    \n",
    "    print(fold_train_metrics)\n",
    "    print(f\"Best val metrics values for fold{fold}\")    \n",
    "    print(fold_val_metrics)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
